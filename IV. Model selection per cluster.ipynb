{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from kneed import KneeLocator\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from collections import Counter\n",
    "from tabulate import tabulate\n",
    "from tsfeatures import tsfeatures\n",
    "from scipy.stats import uniform\n",
    "from mango import Tuner\n",
    "from mango import scheduler\n",
    "from contextlib import contextmanager\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.holtwinters import Holt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure seaborn plot style: set background color and use dark grid\n",
    "sns.set(rc={'axes.facecolor':'#E6E6E6'}, style='darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/train_clustered.csv\", index_col=0)\n",
    "df_test = pd.read_csv(\"data/test_clustered.csv\", index_col=0)\n",
    "cluster = pd.read_csv(\"data/clustered_products.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.ds = pd.to_datetime(df_train.ds, format=\"%Y-%m-%d\")\n",
    "df_test.ds = pd.to_datetime(df_test.ds, format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clusters = df_train.cluster.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Model selection per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if timing file exists\n",
    "if os.path.exists(\"timing.csv\"):\n",
    "    timing = pd.read_csv(\"timing.csv\", index_col=0)\n",
    "else:\n",
    "    timing = pd.DataFrame(columns=['time_started', 'type of run', 'time_ended', 'time_elapsed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mango import Tuner\n",
    "import parameters\n",
    "import importlib\n",
    "\n",
    "importlib.reload(parameters)\n",
    "from parameters import param_spaces, optimizer_configs\n",
    "\n",
    "def evaluate_models_on_centroid(train, test):\n",
    "    results = {}\n",
    "\n",
    "    def objective_function(args_list):\n",
    "        results = []\n",
    "        for params in args_list:\n",
    "            try:\n",
    "                if model == \"SARIMA\":\n",
    "                    order = (params['p'], params['d'], params['q'])\n",
    "                    seasonal_order = (params['P'], params['D'], params['Q'], params['s'])\n",
    "                    model_instance = SARIMAX(train, order=order, seasonal_order=seasonal_order).fit(disp=False)\n",
    "                elif model == \"ExponentialSmoothing\":\n",
    "                    model_instance = ExponentialSmoothing(\n",
    "                        train,\n",
    "                        seasonal=params['seasonal'],\n",
    "                        seasonal_periods=params['seasonal_periods']\n",
    "                    ).fit()\n",
    "                elif model == \"Holt\":\n",
    "                    model_instance = Holt(train, exponential=params['exponential']).fit()\n",
    "                elif model == \"LinearRegression\":\n",
    "                    X_train = np.arange(len(train)).reshape(-1, 1)\n",
    "                    model_instance = LinearRegression()\n",
    "                    model_instance.fit(X_train, train)\n",
    "                    forecast = model_instance.predict(np.arange(len(train), len(train) + len(test)).reshape(-1, 1))\n",
    "                    error = mean_absolute_error(test, forecast)\n",
    "                    results.append(error)\n",
    "                    continue\n",
    "                elif model == \"XGBRegressor\":\n",
    "                    X_train = np.arange(len(train)).reshape(-1, 1)\n",
    "                    model_instance = XGBRegressor(\n",
    "                        n_estimators=params['n_estimators'],\n",
    "                        max_depth=params['max_depth'],\n",
    "                        learning_rate=params['learning_rate'],\n",
    "                        objective='reg:squarederror',\n",
    "                        random_state=42\n",
    "                    )\n",
    "                    model_instance.fit(X_train, train)\n",
    "                    forecast = model_instance.predict(np.arange(len(train), len(train) + len(test)).reshape(-1, 1))\n",
    "                    error = mean_absolute_error(test, forecast)\n",
    "                    results.append(error)\n",
    "                    continue\n",
    "                elif model == \"ARIMA\":\n",
    "                    order = (params['p'], params['d'], params['q'])\n",
    "                    model_instance = ARIMA(train, order=order).fit()\n",
    "                elif model == \"RandomForestRegressor\":\n",
    "                    X_train = np.arange(len(train)).reshape(-1, 1)\n",
    "                    model_instance = RandomForestRegressor(\n",
    "                        n_estimators=params['n_estimators'],\n",
    "                        max_depth=params['max_depth'],\n",
    "                        random_state=42\n",
    "                    )\n",
    "                    model_instance.fit(X_train, train)\n",
    "                    forecast = model_instance.predict(np.arange(len(train), len(train) + len(test)).reshape(-1, 1))\n",
    "                    error = mean_absolute_error(test, forecast)\n",
    "                    results.append(error)\n",
    "                    continue\n",
    "                elif model == \"LSTM\":\n",
    "                    model_instance = Sequential([\n",
    "                        LSTM(params['units'], activation='relu', input_shape=(1, 1)),\n",
    "                        Dense(1)\n",
    "                    ])\n",
    "                    model_instance.compile(optimizer='adam', loss='mae')\n",
    "                    train_reshaped = train.values.reshape(-1, 1, 1)\n",
    "                    model_instance.fit(train_reshaped, train, epochs=params['epochs'], batch_size=1, verbose=0)\n",
    "                    test_reshaped = np.arange(len(train), len(train) + len(test)).reshape(-1, 1, 1)\n",
    "                    forecast = model_instance.predict(test_reshaped).flatten()\n",
    "                    error = mean_absolute_error(test, forecast)\n",
    "                    results.append(error)\n",
    "                    continue\n",
    "\n",
    "                forecast = model_instance.forecast(steps=len(test))\n",
    "                error = mean_absolute_error(test, forecast)\n",
    "                results.append(error)\n",
    "            except Exception as e:\n",
    "                results.append(100000000)  # Assign a high error value for exceptions\n",
    "        return results\n",
    "\n",
    "    # SARIMA\n",
    "    try:\n",
    "        model = \"SARIMA\"\n",
    "        param_space = param_spaces[model]\n",
    "        optimizer_config = optimizer_configs[model]\n",
    "        tuner = Tuner(param_space, objective_function, optimizer_config)\n",
    "        sarima_results = tuner.minimize()\n",
    "\n",
    "        best_params = sarima_results['best_params']\n",
    "        best_order = (best_params['p'], best_params['d'], best_params['q'])\n",
    "        best_seasonal_order = (best_params['P'], best_params['D'], best_params['Q'], best_params['s'])\n",
    "        sarima_model = SARIMAX(train, order=best_order, seasonal_order=best_seasonal_order).fit(disp=False)\n",
    "        sarima_forecast = sarima_model.forecast(steps=len(test))\n",
    "        results['SARIMA'] = (mean_absolute_error(test, sarima_forecast), best_params)\n",
    "    except Exception as e:\n",
    "        results['SARIMA'] = (float('inf'), None)\n",
    "\n",
    "    # Exponential Smoothing\n",
    "    try:\n",
    "        model = \"ExponentialSmoothing\"\n",
    "        param_space = param_spaces[model]\n",
    "        optimizer_config = optimizer_configs[model]\n",
    "        tuner = Tuner(param_space, objective_function, optimizer_config)\n",
    "        es_results = tuner.minimize()\n",
    "\n",
    "        best_params = es_results['best_params']\n",
    "        es_model = ExponentialSmoothing(\n",
    "            train,\n",
    "            seasonal=best_params['seasonal'],\n",
    "            seasonal_periods=best_params['seasonal_periods']\n",
    "        ).fit()\n",
    "        es_forecast = es_model.forecast(steps=len(test))\n",
    "        results['Exponential Smoothing'] = (mean_absolute_error(test, es_forecast), best_params)\n",
    "    except Exception as e:\n",
    "        results['Exponential Smoothing'] = (float('inf'), None)\n",
    "\n",
    "    # Holt's Linear Trend\n",
    "    try:\n",
    "        model = \"Holt\"\n",
    "        param_space = param_spaces[model]\n",
    "        optimizer_config = optimizer_configs[model]\n",
    "        tuner = Tuner(param_space, objective_function, optimizer_config)\n",
    "        holt_results = tuner.minimize()\n",
    "\n",
    "        best_params = holt_results['best_params']\n",
    "        holt_model = Holt(train, exponential=best_params['exponential']).fit()\n",
    "        holt_forecast = holt_model.forecast(steps=len(test))\n",
    "        results[\"Holt's Linear Trend\"] = (mean_absolute_error(test, holt_forecast), best_params)\n",
    "    except Exception as e:\n",
    "        results[\"Holt's Linear Trend\"] = (float('inf'), None)\n",
    "\n",
    "    # XGBoost\n",
    "    try:\n",
    "        model = \"XGBRegressor\"\n",
    "        param_space = param_spaces[model]\n",
    "        optimizer_config = optimizer_configs[model]\n",
    "        tuner = Tuner(param_space, objective_function, optimizer_config)\n",
    "        xgb_results = tuner.minimize()\n",
    "\n",
    "        best_params = xgb_results['best_params']\n",
    "        X_train = np.arange(len(train)).reshape(-1, 1)\n",
    "        X_test = np.arange(len(train), len(train) + len(test)).reshape(-1, 1)\n",
    "        xgb_model = XGBRegressor(\n",
    "            n_estimators=best_params['n_estimators'],\n",
    "            max_depth=best_params['max_depth'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            objective='reg:squarederror',\n",
    "            random_state=42\n",
    "        )\n",
    "        xgb_model.fit(X_train, train)\n",
    "        xgb_forecast = xgb_model.predict(X_test)\n",
    "        results['XGBoost'] = (mean_absolute_error(test, xgb_forecast), best_params)\n",
    "    except Exception as e:\n",
    "        results['XGBoost'] = (float('inf'), None)\n",
    "\n",
    "    # ARIMA\n",
    "    try:\n",
    "        model = \"ARIMA\"\n",
    "        param_space = param_spaces[model]\n",
    "        optimizer_config = optimizer_configs[model]\n",
    "        tuner = Tuner(param_space, objective_function, optimizer_config)\n",
    "        arima_results = tuner.minimize()\n",
    "\n",
    "        best_params = arima_results['best_params']\n",
    "        best_order = (best_params['p'], best_params['d'], best_params['q'])\n",
    "        arima_model = ARIMA(train, order=best_order).fit()\n",
    "        arima_forecast = arima_model.forecast(steps=len(test))\n",
    "        results['ARIMA'] = (mean_absolute_error(test, arima_forecast), best_params)\n",
    "    except Exception as e:\n",
    "        results['ARIMA'] = (float('inf'), None)\n",
    "\n",
    "    # Linear Regression\n",
    "    try:\n",
    "        model = \"LinearRegression\"\n",
    "        param_space = {}  # No hyperparameters to tune for Linear Regression\n",
    "        optimizer_config = {'initial_random': 1, 'num_iteration': 1}\n",
    "        tuner = Tuner(param_space, objective_function, optimizer_config)\n",
    "        lr_results = tuner.minimize()\n",
    "\n",
    "        X_train = np.arange(len(train)).reshape(-1, 1)\n",
    "        lr_model = LinearRegression()\n",
    "        lr_model.fit(X_train, train)\n",
    "        X_test = np.arange(len(train), len(train) + len(test)).reshape(-1, 1)\n",
    "        lr_forecast = lr_model.predict(X_test)\n",
    "        results['Linear Regression'] = (mean_absolute_error(test, lr_forecast), None)\n",
    "    except Exception as e:\n",
    "        results['Linear Regression'] = (float('inf'), None)\n",
    "\n",
    "    # Random Forest\n",
    "    try:\n",
    "        model = \"RandomForestRegressor\"\n",
    "        param_space = param_spaces[model]\n",
    "        optimizer_config = optimizer_configs[model]\n",
    "        tuner = Tuner(param_space, objective_function, optimizer_config)\n",
    "        rf_results = tuner.minimize()\n",
    "\n",
    "        best_params = rf_results['best_params']\n",
    "        X_train = np.arange(len(train)).reshape(-1, 1)\n",
    "        X_test = np.arange(len(train), len(train) + len(test)).reshape(-1, 1)\n",
    "        rf_model = RandomForestRegressor(\n",
    "            n_estimators=best_params['n_estimators'],\n",
    "            max_depth=best_params['max_depth'],\n",
    "            random_state=42\n",
    "        )\n",
    "        rf_model.fit(X_train, train)\n",
    "        rf_forecast = rf_model.predict(X_test)\n",
    "        results['RandomForestRegressor'] = (mean_absolute_error(test, rf_forecast), best_params)\n",
    "    except Exception as e:\n",
    "        results['RandomForestRegressor'] = (float('inf'), None)\n",
    "\n",
    "    # LSTM\n",
    "    try:\n",
    "        model = \"LSTM\"\n",
    "        param_space = param_spaces[model]\n",
    "        optimizer_config = optimizer_configs[model]\n",
    "        tuner = Tuner(param_space, objective_function, optimizer_config)\n",
    "        lstm_results = tuner.minimize()\n",
    "\n",
    "        best_params = lstm_results['best_params']\n",
    "        lstm_model = Sequential([\n",
    "            LSTM(best_params['units'], activation='relu', input_shape=(1, 1)),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        lstm_model.compile(optimizer='adam', loss='mae')\n",
    "        train_reshaped = train.values.reshape(-1, 1, 1)\n",
    "        lstm_model.fit(train_reshaped, train, epochs=best_params['epochs'], batch_size=1, verbose=0)\n",
    "        test_reshaped = np.arange(len(train), len(train) + len(test)).reshape(-1, 1, 1)\n",
    "        lstm_forecast = lstm_model.predict(test_reshaped).flatten()\n",
    "        results['LSTM'] = (mean_absolute_error(test, lstm_forecast), best_params)\n",
    "    except Exception as e:\n",
    "        results['LSTM'] = (float('inf'), None)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse dictionary\n",
    "def inverse_dict(d):\n",
    "    return {v: k for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def suppress_output():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        old_stderr = sys.stderr\n",
    "        try:\n",
    "            sys.stdout = devnull\n",
    "            sys.stderr = devnull\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "            sys.stderr = old_stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000022C62FAE3E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000022C6423AC00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Cluster 0: Best Model = RandomForestRegressor (MAE = 0.00)\n",
      "Cluster 2: Best Model = ARIMA (MAE = 22.71)\n",
      "Cluster 1: Best Model = SARIMA (MAE = 7.26)\n"
     ]
    }
   ],
   "source": [
    "# Find the best model for each cluster\n",
    "clusters_centroid = inverse_dict(cluster[cluster[\"centroid\"] == True][\"cluster\"].to_dict())\n",
    "clusters_model ={}\n",
    "import time\n",
    "\n",
    "timer = time.time()\n",
    "timing_row_starting = [time.strftime(\"%H:%M:%S\", time.gmtime(time.time())), 'finding clusteroid optimized model', None, None]\n",
    "\n",
    "for c, i in clusters_centroid.items():\n",
    "    train = df_train[df_train[\"unique_id\"] == i].y\n",
    "    test = df_test[df_test[\"unique_id\"] == i].y\n",
    "    with suppress_output():\n",
    "        mae = evaluate_models_on_centroid(train, test)\n",
    "    best_model = min(mae.items(), key=lambda x: x[1][0])\n",
    "    clusters_model[c] = (best_model[0], mae[best_model[0]][1])\n",
    "\n",
    "    print(f\"Cluster {c}: Best Model = {best_model[0]} (MAE = {best_model[1][0]:.2f})\")\n",
    "\n",
    "timing_row_ending = [None, 'finding clusteroid optimized model', time.strftime(\"%H:%M:%S\", time.gmtime(time.time())), time.time() - timer]\n",
    "\n",
    "timing = pd.concat([timing, pd.DataFrame([timing_row_starting, timing_row_ending], columns=timing.columns)])\n",
    "\n",
    "timing.to_csv(\"timing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_model_df = pd.DataFrame(\n",
    "\t[(cluster, model, params) for cluster, (model, params) in clusters_model.items()],\n",
    "\tcolumns=[\"cluster\", \"model\", \"params\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_model_df.to_csv(\"data/clusters_model.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
